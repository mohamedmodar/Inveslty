{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 10969691,
          "sourceType": "datasetVersion",
          "datasetId": 6825480
        },
        {
          "sourceId": 10992966,
          "sourceType": "datasetVersion",
          "datasetId": 6842522
        },
        {
          "sourceId": 11039975,
          "sourceType": "datasetVersion",
          "datasetId": 6876594
        },
        {
          "sourceId": 11050286,
          "sourceType": "datasetVersion",
          "datasetId": 6884141
        },
        {
          "sourceId": 11077938,
          "sourceType": "datasetVersion",
          "datasetId": 6904443
        },
        {
          "sourceId": 11116993,
          "sourceType": "datasetVersion",
          "datasetId": 6931822
        },
        {
          "sourceId": 11120439,
          "sourceType": "datasetVersion",
          "datasetId": 6934512
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8WLP6cjwHND",
        "outputId": "f03c7173-c2e4-4459-859c-b93bddede52a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "area_num = -3"
      ],
      "metadata": {
        "id": "AEosg3h68S6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.decomposition import PCA\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.linear_model import LassoCV\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from joblib import parallel_backend\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from sklearn.utils import resample\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def impute_data(data):\n",
        "    imp = IterativeImputer(max_iter=10, random_state=0)\n",
        "    X = data.drop('Date', axis=1)\n",
        "    data_imputed = pd.DataFrame(imp.fit_transform(X), columns=X.columns)\n",
        "\n",
        "    data_imputed[\"Date\"] = data[\"Date\"]\n",
        "    return data\n",
        "\n",
        "def import_data(ma_window):\n",
        "    macro = pd.read_csv(\"/content/drive/MyDrive/GRAD/macroeconomics.csv\", index_col=0)\n",
        "    areas_prices = pd.read_csv(\"/content/drive/MyDrive/GRAD/ppm_2017_2024_areas_final_v7.csv\", index_col=0)\n",
        "\n",
        "    areas = areas_prices.columns\n",
        "    area = areas[area_num]\n",
        "    areas_prices = areas_prices.reset_index()\n",
        "    areas_prices = areas_prices[[area, \"Date\"]]\n",
        "    areas_prices = areas_prices.melt(id_vars='Date', var_name='Area', value_name='Price Per Meter')\n",
        "\n",
        "    alex = pd.merge(macro, areas_prices, on=\"Date\", how=\"left\")\n",
        "    alex = alex.drop([\"Area\"], axis=1)\n",
        "    alex = alex.dropna(subset=['Price Per Meter'])\n",
        "    alex = alex.drop([\"Alexandria's Real Estate Ownership (GDP) (Thousands EGP)\"], axis=1)\n",
        "    alex['Year'] = alex['Date'].str[:4]\n",
        "    alex['Quarter'] = alex['Date'].str[5:].astype(int)\n",
        "    alex = alex.drop_duplicates(subset=[\"Date\"]).reset_index(drop=True)\n",
        "\n",
        "    og_columns = list(alex.columns)\n",
        "\n",
        "    if ma_window:\n",
        "        for col in alex.columns:\n",
        "            if col in [\"Date\", \"Year\", \"Quarter\", \"Price Per Meter\"]:\n",
        "                continue\n",
        "\n",
        "            ma = col + \"_ma\"\n",
        "            alex[ma] = alex[col].rolling(window=ma_window).mean()\n",
        "\n",
        "        alex = impute_data(alex)\n",
        "\n",
        "    alex.attrs['area'] = area\n",
        "\n",
        "    return alex\n",
        "\n",
        "def select_lags(alex, lags_n):\n",
        "    lags = {}\n",
        "    for col in alex.columns:\n",
        "        if col in [\"Date\", \"Year\", \"Quarter\", \"Price Per Meter\"]:\n",
        "            continue\n",
        "\n",
        "        for i in range(1,lags_n):\n",
        "            lag = col + \"_lag_\" + str(i)\n",
        "            lags[lag] = alex[col].shift(i)\n",
        "\n",
        "    lags_df = pd.concat(lags, axis=1)\n",
        "\n",
        "    X = lags_df\n",
        "    X.fillna(0, inplace=True)\n",
        "    y = alex['Price Per Meter']\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "    lasso = LassoCV(cv=tscv)\n",
        "    lasso.fit(X_scaled, y)\n",
        "\n",
        "    coef = pd.Series(lasso.coef_, index=X.columns)\n",
        "    selected_lags = coef[coef != 0].index.tolist()\n",
        "\n",
        "    lags_df = lags_df.drop(columns=[col for col in lags_df.columns if col not in selected_lags], axis=1)\n",
        "    alex = pd.concat([alex, lags_df], axis=1)\n",
        "    alex.set_index(\"Date\", inplace=True)\n",
        "\n",
        "    return alex\n",
        "\n",
        "def data_preprocessing(alex, pca_comp, tst_size):\n",
        "    # alex[\"Price Per Meter\"] = np.log1p(alex[\"Price Per Meter\"])\n",
        "    # alex[\"Price Per Meter\"] = alex[\"Price Per Meter\"] - alex[\"Price Per Meter\"].shift(1)\n",
        "    # alex = alex.dropna(subset=['Price Per Meter'])\n",
        "    X = alex.drop([\"Price Per Meter\"], axis=1)\n",
        "    y = alex[\"Price Per Meter\"]\n",
        "    print(alex.shape)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=tst_size, shuffle=False)\n",
        "    y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
        "    y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
        "\n",
        "    X_train = create_pca_comp(X_train, pca_comp)\n",
        "    X_test = create_pca_comp(X_test, pca_comp)\n",
        "\n",
        "    X_train[\"Quarter\"] = X_train[\"Quarter\"].astype(int)\n",
        "    X_test[\"Quarter\"] = X_test[\"Quarter\"].astype(int)\n",
        "    X_test[\"Year\"] = X_test[\"Year\"].astype(int)\n",
        "    X_train[\"Year\"] = X_train[\"Year\"].astype(int)\n",
        "\n",
        "    return (X_train, X_test, y_train, y_test)\n",
        "\n",
        "def create_pca_comp(X, pca_comp):\n",
        "    date = X[['Year', 'Quarter']]\n",
        "    pca = PCA(n_components=pca_comp)\n",
        "    X.fillna(0, inplace=True)\n",
        "    X_pca = pca.fit_transform(X.drop([\"Year\", \"Quarter\"], axis=1))\n",
        "\n",
        "    df_pca = pd.DataFrame(X_pca, columns=[f'pca_{i+1}' for i in range(X_pca.shape[1])])\n",
        "    X = pd.concat([date.reset_index(drop=True), df_pca], axis=1)\n",
        "\n",
        "    X.replace(0, np.nan, inplace=True)\n",
        "\n",
        "    return X\n",
        "\n",
        "def scale_data(X_train, X_test, y_train, y_test):\n",
        "    x_scaler = MinMaxScaler()\n",
        "\n",
        "    X_train = pd.DataFrame(x_scaler.fit_transform(X_train),\n",
        "                         columns=X_train.columns,\n",
        "                         index=X_train.index)\n",
        "\n",
        "    X_test = pd.DataFrame(x_scaler.fit_transform(X_test),\n",
        "                         columns=X_test.columns,\n",
        "                         index=X_test.index)\n",
        "\n",
        "    y_scaler = MinMaxScaler()\n",
        "\n",
        "    y_test = y_scaler.fit_transform(y_test)\n",
        "\n",
        "    y_train = y_scaler.fit_transform(y_train)\n",
        "\n",
        "    return (X_train, X_test, y_train, y_test, x_scaler, y_scaler)\n",
        "\n",
        "def select_best_model_params(X_train, y_train):\n",
        "    cv_split = TimeSeriesSplit(n_splits=4)\n",
        "    model = xgb.XGBRegressor()\n",
        "    parameters = {\n",
        "        \"max_depth\": [3, 4, 5, 7],\n",
        "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "        \"n_estimators\": [50, 100, 200]\n",
        "    }\n",
        "\n",
        "    # 'reg_alpha': [0, 0.1, 0.5],\n",
        "    #     'reg_lambda': [0, 0.1, 0.5]\n",
        "\n",
        "    grid_search = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters, scoring='r2', verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search.best_params_\n",
        "\n",
        "def get_params(itrs):\n",
        "    sorted_itrs = sorted(itrs, key=lambda x: x['R2'], reverse=True)\n",
        "    params = sorted_itrs[0]\n",
        "    return params\n",
        "\n",
        "def run_model(X_train, X_test, y_train, y_test, best_model_params):\n",
        "    reg = xgb.XGBRegressor(base_score=0.5, booster='gbtree',\n",
        "                           n_estimators=best_model_params[\"n_estimators\"],\n",
        "                           early_stopping_rounds=10,\n",
        "                           objective='reg:squarederror',\n",
        "                           max_depth=best_model_params[\"max_depth\"],\n",
        "                           learning_rate=best_model_params[\"learning_rate\"],\n",
        "                           random_state=42,\n",
        "                           subsample=1.0,\n",
        "                           reg_alpha=0,\n",
        "                           colsample_bytree=0.7,\n",
        "                           reg_lambda=0.5,\n",
        "                           seed=25)\n",
        "\n",
        "    reg.fit(X_train, y_train,\n",
        "            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "            verbose=False)\n",
        "\n",
        "    pred = reg.predict(X_test)\n",
        "    r2 = r2_score(y_test, pred)\n",
        "\n",
        "    return (r2, reg)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-21T22:14:57.857751Z",
          "iopub.execute_input": "2025-03-21T22:14:57.858190Z",
          "iopub.status.idle": "2025-03-21T22:14:57.882993Z",
          "shell.execute_reply.started": "2025-03-21T22:14:57.858158Z",
          "shell.execute_reply": "2025-03-21T22:14:57.880958Z"
        },
        "id": "y0PjTXGuX0lz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def get_future_data(params):\n",
        "    macro_test = pd.read_csv(\"/content/drive/MyDrive/GRAD/macro_pred_2025_2028.csv\", index_col=0)\n",
        "\n",
        "    macro_test.index.name = \"Date\"\n",
        "\n",
        "    macro_test = macro_test.rename(columns={\"Real Estate Activitie\":\"Egypt's Real Estate Activities (GDP) (Millions EGP)\",\n",
        "                              \"Points\":\"Aqarmap Index\"})\n",
        "\n",
        "    macro_test_lags = {}\n",
        "    for col in macro_test.columns:\n",
        "        for i in range(1, params[\"LAGS\"]):\n",
        "            lag = col + \"_lag_\" + str(i)\n",
        "            macro_test_lags[lag] = macro_test[col].shift(i)\n",
        "\n",
        "    macro_test_lags_df = pd.concat(macro_test_lags, axis=1)\n",
        "\n",
        "    macro_test = pd.concat([macro_test, macro_test_lags_df], axis=1)\n",
        "\n",
        "    macro_test['Year'] = macro_test.index.str[:4].astype(int)\n",
        "    macro_test['Quarter'] = macro_test.index.str[5:].astype(int)\n",
        "\n",
        "    macro_test.fillna(0, inplace=True)\n",
        "\n",
        "    pca = PCA(n_components=params[\"PCA\"])\n",
        "    macro_test_pca = pca.fit_transform(macro_test.drop([\"Year\", \"Quarter\"], axis=1))\n",
        "\n",
        "    df_pca = pd.DataFrame(macro_test_pca, columns=[f'pca_{i+1}' for i in range(macro_test_pca.shape[1])])\n",
        "    macro_test_pca = pd.concat([macro_test[['Year', 'Quarter']].reset_index(drop=True), df_pca], axis=1)\n",
        "\n",
        "    macro_test_pca.replace(0, np.nan, inplace=True)\n",
        "\n",
        "    macro_test_pca = pd.DataFrame(x_scaler.fit_transform(macro_test_pca),\n",
        "                         columns=macro_test_pca.columns,\n",
        "                         index=macro_test_pca.index)\n",
        "\n",
        "    return (macro_test_pca, macro_test)\n",
        "\n",
        "def run_model_ci(X_train, X_test, y_train, y_test, best_model_params, future_data):\n",
        "    n_bootstraps = 1000\n",
        "    preds_bootstrap = []\n",
        "\n",
        "    for i in range(n_bootstraps):\n",
        "        X_resampled, y_resampled = resample(X_train, y_train, replace=True, random_state=i)\n",
        "\n",
        "        model = xgb.XGBRegressor(base_score=0.5, booster='gbtree',\n",
        "                           n_estimators=best_model_params[\"n_estimators\"],\n",
        "                           objective='reg:squarederror',\n",
        "                           max_depth=best_model_params[\"max_depth\"],\n",
        "                           learning_rate=best_model_params[\"learning_rate\"],\n",
        "                           random_state=42,\n",
        "                           subsample=1.0,\n",
        "                           reg_alpha=0,\n",
        "                           colsample_bytree=0.7,\n",
        "                           reg_lambda=0.5,\n",
        "                           seed=25)\n",
        "\n",
        "        model.fit(X_resampled, y_resampled)\n",
        "\n",
        "        preds = model.predict(future_data)\n",
        "        preds_bootstrap.append(y_scaler.inverse_transform([preds]))\n",
        "\n",
        "    preds_bootstrap = np.array(preds_bootstrap)\n",
        "\n",
        "    mean_preds = preds_bootstrap.mean(axis=0)\n",
        "    std_preds = preds_bootstrap.std(axis=0)\n",
        "\n",
        "    lower_bound = mean_preds - 1.44 * std_preds\n",
        "    upper_bound = mean_preds + 1.44 * std_preds\n",
        "\n",
        "    return (lower_bound, upper_bound, mean_preds)\n",
        "\n",
        "def get_ci(X_train, X_test, y_train, y_test, best_model_params, future_data_pca):\n",
        "    (lower_bound, upper_bound, means_preds) = run_model_ci(X_train, X_test, y_train, y_test, best_model_params, future_data_pca)\n",
        "    means_preds, lower_bound, upper_bound = means_preds.flatten() * 2.43, lower_bound.flatten() * 2.43, upper_bound.flatten() * 2.43\n",
        "    return (lower_bound, upper_bound, means_preds)\n",
        "\n",
        "def get_ci_chart(lower_bound, upper_bound, means_preds, area, q_labels):\n",
        "    plt.plot(means_preds, label='Mean Prediction')\n",
        "    plt.xticks(ticks=range(len(means_preds)), labels=q_labels, rotation=30)\n",
        "    plt.fill_between(range(len(means_preds)), lower_bound, upper_bound, alpha=0.3, label='Confidence Interval')\n",
        "    plt.title(area)\n",
        "    plt.legend()\n",
        "    chart_title = area + \"-forecast.png\"\n",
        "    plt.savefig(chart_title, dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-21T22:11:47.561302Z",
          "iopub.execute_input": "2025-03-21T22:11:47.561677Z",
          "iopub.status.idle": "2025-03-21T22:11:47.577579Z",
          "shell.execute_reply.started": "2025-03-21T22:11:47.561647Z",
          "shell.execute_reply": "2025-03-21T22:11:47.575861Z"
        },
        "id": "hPAG3ez9X0l4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "itrs = []\n",
        "for size in np.arange(0.05, 0.25, 0.05):\n",
        "    for ma_window in [0, 4, 8, 12]:\n",
        "        for i in range(4, 21, 4):\n",
        "            alex = import_data(ma_window)\n",
        "            alex = select_lags(alex, i)\n",
        "            print(alex.attrs)\n",
        "            loop_pca_untill = int(size * alex.shape[0]) + 1\n",
        "            for comp in range(5, loop_pca_untill):\n",
        "                print(\"----------------------\")\n",
        "                (X_train, X_test, y_train, y_test) = data_preprocessing(alex, comp, size)\n",
        "                (X_train, X_test, y_train, y_test, x_scaler, y_scaler) = scale_data(X_train, X_test, y_train, y_test)\n",
        "\n",
        "                # if len(best_model_params) == 0:\n",
        "                print(\"Getting best model parameters\")\n",
        "                best_model_params = select_best_model_params(X_train, y_train)\n",
        "                print(best_model_params)\n",
        "                (r2, reg) = run_model(X_train, X_test, y_train, y_test, best_model_params)\n",
        "\n",
        "                itrs.append({\"Test-Size\": size, \"MOVING_AVG\": ma_window, \"LAGS\": i, \"PCA\": comp, \"R2\": r2} | best_model_params)\n",
        "                print(\"Test-Size=\" + str(size) + \" - MOVING_AVG=\" + str(ma_window) + \" - LAGS=\" + str(i) + \" - PCA=\" + str(comp) + \" - R2=\" + str(r2))\n",
        "            print(\"----------------------\\n\")\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "minutes = int(elapsed_time // 60)\n",
        "seconds = int(elapsed_time % 60)\n",
        "duration = str(minutes) + \":\" + str(seconds)\n",
        "print(\"\\nTime to find best params = \" + duration)\n",
        "\n",
        "params = get_params(itrs)\n",
        "print(\"Best params : \" + str(params))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-21T22:15:07.820412Z",
          "iopub.execute_input": "2025-03-21T22:15:07.820766Z",
          "iopub.status.idle": "2025-03-21T22:17:25.715667Z",
          "shell.execute_reply.started": "2025-03-21T22:15:07.820740Z",
          "shell.execute_reply": "2025-03-21T22:17:25.714711Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ePVdjY2X0l5",
        "outputId": "d4d6268e-db47-43f3-a24d-c787d1cebbb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{}\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "(32, 18)\n",
            "Getting best model parameters\n",
            "Fitting 4 folds for each of 60 candidates, totalling 240 fits\n",
            "{'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 100}\n",
            "Test-Size=0.2 - MOVING_AVG=0 - LAGS=4 - PCA=5 - R2=0.29641766141895065\n",
            "----------------------\n",
            "(32, 18)\n",
            "Getting best model parameters\n",
            "Fitting 4 folds for each of 60 candidates, totalling 240 fits\n",
            "{'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 100}\n",
            "Test-Size=0.2 - MOVING_AVG=0 - LAGS=4 - PCA=6 - R2=0.4312813114820311\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "(32, 32)\n",
            "Getting best model parameters\n",
            "Fitting 4 folds for each of 60 candidates, totalling 240 fits\n",
            "{'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 100}\n",
            "Test-Size=0.2 - MOVING_AVG=0 - LAGS=8 - PCA=5 - R2=0.38476443436510743\n",
            "----------------------\n",
            "(32, 32)\n",
            "Getting best model parameters\n",
            "Fitting 4 folds for each of 60 candidates, totalling 240 fits\n",
            "{'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 100}\n",
            "Test-Size=0.2 - MOVING_AVG=0 - LAGS=8 - PCA=6 - R2=0.6773285462849584\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "(32, 30)\n",
            "Getting best model parameters\n",
            "Fitting 4 folds for each of 60 candidates, totalling 240 fits\n",
            "{'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 100}\n",
            "Test-Size=0.2 - MOVING_AVG=0 - LAGS=12 - PCA=5 - R2=0.36740589060952356\n",
            "----------------------\n",
            "(32, 30)\n",
            "Getting best model parameters\n",
            "Fitting 4 folds for each of 60 candidates, totalling 240 fits\n",
            "{'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 100}\n",
            "Test-Size=0.2 - MOVING_AVG=0 - LAGS=12 - PCA=6 - R2=0.5040075174725194\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "(32, 42)\n",
            "Getting best model parameters\n",
            "Fitting 4 folds for each of 60 candidates, totalling 240 fits\n",
            "{'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 100}\n",
            "Test-Size=0.2 - MOVING_AVG=0 - LAGS=16 - PCA=5 - R2=0.48809377228655837\n",
            "----------------------\n",
            "(32, 42)\n",
            "Getting best model parameters\n",
            "Fitting 4 folds for each of 60 candidates, totalling 240 fits\n",
            "{'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 100}\n",
            "Test-Size=0.2 - MOVING_AVG=0 - LAGS=16 - PCA=6 - R2=0.6321218993467039\n",
            "----------------------\n",
            "\n",
            "{'area': 'hay-el-gomrok'}\n",
            "----------------------\n",
            "(32, 42)\n",
            "Getting best model parameters\n",
            "Fitting 4 folds for each of 60 candidates, totalling 240 fits\n",
            "{'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 100}\n",
            "Test-Size=0.2 - MOVING_AVG=0 - LAGS=20 - PCA=5 - R2=0.49799392700553924\n",
            "----------------------\n",
            "(32, 42)\n",
            "Getting best model parameters\n",
            "Fitting 4 folds for each of 60 candidates, totalling 240 fits\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-21T21:51:01.955512Z",
          "iopub.status.idle": "2025-03-21T21:51:01.955880Z",
          "shell.execute_reply": "2025-03-21T21:51:01.955714Z"
        },
        "id": "rrJfbeEmX0l7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "(future_data_pca, future_data) = get_future_data(params)\n",
        "\n",
        "alex = import_data(params[\"MOVING_AVG\"])\n",
        "alex = select_lags(alex, params[\"LAGS\"])\n",
        "(X_train, X_test, y_train, y_test) = data_preprocessing(alex, params[\"PCA\"], params[\"Test-Size\"])\n",
        "(X_train, X_test, y_train, y_test, x_scaler, y_scaler) = scale_data(X_train, X_test, y_train, y_test)\n",
        "# (r2, reg) = run_model(X_train, X_test, y_train, y_test, best_model_params)\n",
        "# print(r2)\n",
        "(lower_bound, upper_bound, means_preds) = get_ci(X_train, X_test, y_train, y_test, params, future_data_pca)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-21T22:25:06.977915Z",
          "iopub.execute_input": "2025-03-21T22:25:06.979313Z",
          "iopub.status.idle": "2025-03-21T22:25:41.482646Z",
          "shell.execute_reply.started": "2025-03-21T22:25:06.979246Z",
          "shell.execute_reply": "2025-03-21T22:25:41.481423Z"
        },
        "id": "HStwxmX9X0l7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "q_labels = list(future_data.index[-len(means_preds):])\n",
        "get_ci_chart(lower_bound, upper_bound, means_preds, alex.attrs['area'], q_labels)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-21T21:51:01.957962Z",
          "iopub.status.idle": "2025-03-21T21:51:01.958469Z",
          "shell.execute_reply": "2025-03-21T21:51:01.958250Z"
        },
        "id": "DS72ksCnX0l8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "means_preds = list(means_preds)\n",
        "for i in range(len(means_preds)):\n",
        "    print(q_labels[i] + \" => (\" + str(round(lower_bound[i], 2)) + \", \" + str(round(upper_bound[i], 2)) + \") L.E.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-21T21:51:01.959721Z",
          "iopub.status.idle": "2025-03-21T21:51:01.960246Z",
          "shell.execute_reply": "2025-03-21T21:51:01.960022Z"
        },
        "id": "usZg7sMOX0l8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "forecast = pd.DataFrame()\n",
        "forecast[\"Date\"] = q_labels\n",
        "forecast[\"Lower Bound\"] = lower_bound\n",
        "forecast[\"Upper Bound\"] = upper_bound\n",
        "forecast[\"Point Estimation\"] = means_preds"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-21T21:51:01.961476Z",
          "iopub.status.idle": "2025-03-21T21:51:01.961976Z",
          "shell.execute_reply": "2025-03-21T21:51:01.961770Z"
        },
        "id": "FNajzrv4X0l9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_df_path = alex.attrs['area'] + \"-forecast\" + \".csv\"\n",
        "forecast.to_csv(forecast_df_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-21T21:51:01.963344Z",
          "iopub.status.idle": "2025-03-21T21:51:01.963686Z",
          "shell.execute_reply": "2025-03-21T21:51:01.963556Z"
        },
        "id": "uwjj-8y0X0l9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "params_path = alex.attrs['area'] + \"-params\" + \".json\"\n",
        "with open(params_path, \"w\") as f:\n",
        "    json.dump(params, f, indent=4)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-21T21:51:01.964397Z",
          "iopub.status.idle": "2025-03-21T21:51:01.964721Z",
          "shell.execute_reply": "2025-03-21T21:51:01.964596Z"
        },
        "id": "mJJY-hdbX0l-"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}